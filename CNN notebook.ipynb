{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import os, shutil\n",
    "import utils\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from ML_algorithms import *\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"data_files/Cactus_Image/training_set\"\n",
    "test_dir = \"data_files/Cactus_Image/testing_set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_cnn(n_neurons=32, n_layers=3, filter_size=(3, 3), activation=\"relu\", \n",
    "               input_shape =(64,64,3), max_pooling=(2,2), dense_layer=128, \n",
    "               loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=\"acc\"):\n",
    "    # NOTE: always alter the input_shape to the specific input shape off the problem.\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(n_neurons, filter_size, activation=activation,\n",
    "                           input_shape =input_shape))\n",
    "    model.add(layers.MaxPooling2D(max_pooling))\n",
    "    for num in range(n_layers-2):\n",
    "        model.add(layers.Conv2D(n_neurons, filter_size, activation=activation))\n",
    "        model.add(layers.MaxPooling2D(max_pooling))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(dense_layer, activation=activation))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[metrics])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = keras_cnn() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(64,64),\n",
    "    batch_size=20,\n",
    "    class_mode=\"binary\")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=20,\n",
    "    class_mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=250,\n",
    "    epochs=5,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Timeseries Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = utils.get_timeseries_dataset(cnn_or_lstm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = keras_cnn_conv1D(metrics=\"binary_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=10, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Titanic Data \n",
    "### 2.1 conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:392: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n",
      "/home/jovyan/work/2_Semester/Deep Learning/DLProject/utils.py:97: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  X_train = X_train.as_matrix().reshape((len(X_train), 36))\n",
      "/home/jovyan/work/2_Semester/Deep Learning/DLProject/utils.py:98: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  y_train = y_train.as_matrix().reshape((len(y_train), 1))\n",
      "/home/jovyan/work/2_Semester/Deep Learning/DLProject/utils.py:99: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  X_test = X_test.as_matrix().reshape((len(X_test), 36))\n",
      "/home/jovyan/work/2_Semester/Deep Learning/DLProject/utils.py:100: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  y_test = y_test.as_matrix().reshape((len(y_test), 1))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = utils.get_titanic_dataset(cnn_or_lstm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1047, 1, 36)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras_cnn_conv1D(filters=6, input_shape=(1,36), pool_size=1,kernel_size=1, metrics=\"binary_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1047 samples, validate on 262 samples\n",
      "Epoch 1/10\n",
      "1047/1047 [==============================] - 0s 294us/step - loss: 3.5151 - binary_accuracy: 0.4327 - val_loss: 2.3079 - val_binary_accuracy: 0.5611\n",
      "Epoch 2/10\n",
      "1047/1047 [==============================] - 0s 43us/step - loss: 2.2064 - binary_accuracy: 0.5224 - val_loss: 2.0312 - val_binary_accuracy: 0.5115\n",
      "Epoch 3/10\n",
      "1047/1047 [==============================] - 0s 41us/step - loss: 1.4674 - binary_accuracy: 0.5750 - val_loss: 0.9915 - val_binary_accuracy: 0.6450\n",
      "Epoch 4/10\n",
      "1047/1047 [==============================] - 0s 43us/step - loss: 1.0460 - binary_accuracy: 0.6199 - val_loss: 1.1752 - val_binary_accuracy: 0.6412\n",
      "Epoch 5/10\n",
      "1047/1047 [==============================] - 0s 39us/step - loss: 0.9865 - binary_accuracy: 0.6285 - val_loss: 0.9161 - val_binary_accuracy: 0.6641\n",
      "Epoch 6/10\n",
      "1047/1047 [==============================] - 0s 43us/step - loss: 0.9399 - binary_accuracy: 0.6418 - val_loss: 0.9184 - val_binary_accuracy: 0.6679\n",
      "Epoch 7/10\n",
      "1047/1047 [==============================] - 0s 42us/step - loss: 0.9377 - binary_accuracy: 0.6332 - val_loss: 0.8963 - val_binary_accuracy: 0.6718\n",
      "Epoch 8/10\n",
      "1047/1047 [==============================] - 0s 42us/step - loss: 0.9166 - binary_accuracy: 0.6313 - val_loss: 0.8920 - val_binary_accuracy: 0.6679\n",
      "Epoch 9/10\n",
      "1047/1047 [==============================] - 0s 42us/step - loss: 0.8793 - binary_accuracy: 0.6399 - val_loss: 0.8122 - val_binary_accuracy: 0.6756\n",
      "Epoch 10/10\n",
      "1047/1047 [==============================] - 0s 51us/step - loss: 0.8878 - binary_accuracy: 0.6390 - val_loss: 0.8882 - val_binary_accuracy: 0.6756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f19e4441f60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:392: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = utils.get_titanic_dataset(cnn_conv2d=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = keras_cnn(filter_size=(1,1), input_shape=(1, 36, 1), max_pooling=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1047 samples, validate on 262 samples\n",
      "Epoch 1/10\n",
      "1047/1047 [==============================] - 1s 483us/step - loss: 0.6058 - acc: 0.6848 - val_loss: 0.5864 - val_acc: 0.6947\n",
      "Epoch 2/10\n",
      "1047/1047 [==============================] - 0s 124us/step - loss: 0.5136 - acc: 0.7689 - val_loss: 0.4909 - val_acc: 0.7366\n",
      "Epoch 3/10\n",
      "1047/1047 [==============================] - 0s 133us/step - loss: 0.4872 - acc: 0.7832 - val_loss: 0.4725 - val_acc: 0.7863\n",
      "Epoch 4/10\n",
      "1047/1047 [==============================] - 0s 130us/step - loss: 0.4620 - acc: 0.7966 - val_loss: 0.4608 - val_acc: 0.7977\n",
      "Epoch 5/10\n",
      "1047/1047 [==============================] - 0s 126us/step - loss: 0.4548 - acc: 0.8004 - val_loss: 0.4711 - val_acc: 0.7939\n",
      "Epoch 6/10\n",
      "1047/1047 [==============================] - 0s 124us/step - loss: 0.4576 - acc: 0.8023 - val_loss: 0.4566 - val_acc: 0.7863\n",
      "Epoch 7/10\n",
      "1047/1047 [==============================] - 0s 131us/step - loss: 0.4471 - acc: 0.8013 - val_loss: 0.4560 - val_acc: 0.8015\n",
      "Epoch 8/10\n",
      "1047/1047 [==============================] - 0s 119us/step - loss: 0.4741 - acc: 0.7947 - val_loss: 0.5376 - val_acc: 0.7557\n",
      "Epoch 9/10\n",
      "1047/1047 [==============================] - 0s 129us/step - loss: 0.4521 - acc: 0.7994 - val_loss: 0.4512 - val_acc: 0.7901\n",
      "Epoch 10/10\n",
      "1047/1047 [==============================] - 0s 125us/step - loss: 0.4379 - acc: 0.8138 - val_loss: 0.4638 - val_acc: 0.7863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f19ec4c7048>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_rows, img_cols = 1,36\n",
    "#nb_filters = 1000\n",
    "#pool_size = (1, 1)\n",
    "#kernel_size = (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation, Dropout, Convolution2D, MaxPooling2D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(nb_filters, filter_size=(1,1),\n",
    "                        border_mode='valid',\n",
    "                        input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='mean_squared_error', optimizer='Nadam', metrics=['binary_accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_onehot, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation, Dropout, Convolution2D, MaxPooling2D, Flatten\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, filter_size=(1,1), activation=\"relu\", input_shape=input_shape))\n",
    "model.add(layers.MaxPooling2D(pool_size))\n",
    "model.add(layers.Conv2D(32, filter_size=(1,1), activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(pool_size))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(dense_layer, activation=activation))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='Nadam', metrics=['binary_accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_onehot, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.as_matrix().reshape((len(X_train), 15))\n",
    "y_train = y_train.as_matrix().reshape((len(y_train), 1))\n",
    "X_test = X_test.as_matrix().reshape((len(X_test), 15))\n",
    "y_test = y_test.as_matrix().reshape((len(y_test), 1))\n",
    "\n",
    "train_dataset=hstack((X_train,y_train))\n",
    "test_dataset=hstack((X_test,y_test))\n",
    "\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "X_train, y_train = split_sequences(train_dataset, 3)\n",
    "X_test, y_test = split_sequences(test_dataset, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_cnn_conv1D(filters=64, n_layers=2, kernel_size=2, activation=\"relu\", \n",
    "               input_shape =(3,15), pool_size=2, dense_layer=50, \n",
    "               loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=\"acc\"):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=activation, input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    for num in range(n_layers-2):\n",
    "        model.add(Conv1D(filters=filters, kernel_size=kernen_size, activation=activation, input_shape=input_shape))\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dense_layer, activation=activation))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=[metrics])\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
