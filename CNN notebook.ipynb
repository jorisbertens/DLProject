{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"data_files/Cactus_Image/training_set\"\n",
    "test_dir = \"data_files/Cactus_Image/testing_set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_cnn(n_neurons=32, n_layers=3, filter_size=(3, 3), activation=\"relu\", \n",
    "               input_shape =(64,64,3), max_pooling=(2,2), dense_layer=128, \n",
    "               loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=\"acc\"):\n",
    "    # NOTE: always alter the input_shape to the specific input shape off the problem.\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(n_neurons, filter_size, activation=activation,\n",
    "                           input_shape =input_shape))\n",
    "    model.add(layers.MaxPooling2D(max_pooling))\n",
    "    for num in range(n_layers-2):\n",
    "        model.add(layers.Conv2D(n_neurons, filter_size, activation=activation))\n",
    "        model.add(layers.MaxPooling2D(max_pooling))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(dense_layer, activation=activation))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[metrics])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 813,217\n",
      "Trainable params: 813,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras_cnn() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13999 images belonging to 2 classes.\n",
      "Found 3501 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(64,64),\n",
    "    batch_size=20,\n",
    "    class_mode=\"binary\")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=20,\n",
    "    class_mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 0.2214 - acc: 0.9126 - val_loss: 0.2688 - val_acc: 0.8840\n",
      "Epoch 2/5\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 0.1221 - acc: 0.9560 - val_loss: 0.0509 - val_acc: 0.9820\n",
      "Epoch 3/5\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 0.0863 - acc: 0.9694 - val_loss: 0.2549 - val_acc: 0.8860\n",
      "Epoch 4/5\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 0.0792 - acc: 0.9710 - val_loss: 0.1020 - val_acc: 0.9725\n",
      "Epoch 5/5\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.0690 - acc: 0.9762 - val_loss: 0.0409 - val_acc: 0.9880\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=250,\n",
    "    epochs=5,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import shutil\n",
    "from os import path, listdir\n",
    "\n",
    "\n",
    "def list_dirs(directory):\n",
    "    \"\"\"Returns all directories in a given directory\n",
    "    \"\"\"\n",
    "    return [f for f in pathlib.Path(directory).iterdir() if f.is_dir()]\n",
    "\n",
    "\n",
    "def list_files(directory):\n",
    "    \"\"\"Returns all files in a given directory\n",
    "    \"\"\"\n",
    "    return [f for f in pathlib.Path(directory).iterdir() if f.is_file() and not f.name.startswith('.')]\n",
    "\n",
    "\n",
    "def ratio(input, output=\"output\", seed=1337, ratio=(.8, .1, .1)):\n",
    "    assert sum(ratio) == 1\n",
    "    assert len(ratio) in (2, 3)\n",
    "\n",
    "    for class_dir in list_dirs(input):\n",
    "        split_class_dir_ratio(class_dir, output, ratio, seed)\n",
    "\n",
    "\n",
    "def fixed(input, output=\"output\", seed=1337, fixed=(100, 100), oversample=False):\n",
    "    # make sure its reproducible\n",
    "    if isinstance(fixed, int):\n",
    "        fixed = (fixed)\n",
    "\n",
    "    assert len(fixed) in (1, 2)\n",
    "\n",
    "    dirs = list_dirs(input)\n",
    "    lens = []\n",
    "    for class_dir in dirs:\n",
    "        lens.append(split_class_dir_fixed(class_dir, output, fixed, seed))\n",
    "\n",
    "    if not oversample:\n",
    "        return\n",
    "\n",
    "    max_len = max(lens)\n",
    "\n",
    "    for length, class_dir in zip(lens, dirs):\n",
    "        class_name = path.split(class_dir)[1]\n",
    "        full_path = path.join(output, 'train', class_name)\n",
    "        train_files = list_files(full_path)\n",
    "        for i in range(max_len - length):\n",
    "            f_orig = random.choice(train_files)\n",
    "            new_name = f_orig.stem + '_' + str(i) + f_orig.suffix\n",
    "            f_dest = f_orig.with_name(new_name)\n",
    "            shutil.copy2(f_orig, f_dest)\n",
    "\n",
    "\n",
    "def setup_files(class_dir, seed):\n",
    "    \"\"\"Returns shuffled files\n",
    "    \"\"\"\n",
    "    # make sure its reproducible\n",
    "    random.seed(seed)\n",
    "\n",
    "    files = list_files(class_dir)\n",
    "\n",
    "    files.sort()\n",
    "    random.shuffle(files)\n",
    "    return files\n",
    "\n",
    "\n",
    "def split_class_dir_fixed(class_dir, output, fixed, seed):\n",
    "    \"\"\"Splits one very class folder\n",
    "    \"\"\"\n",
    "    files = setup_files(class_dir, seed)\n",
    "\n",
    "    if not len(files) > sum(fixed):\n",
    "        raise ValueError(f'The number of samples in class \"{class_dir.stem}\" are too few. There are only {len(files)} samples available but your fixed parameter {fixed} requires at least {sum(fixed)} files. You may want to split your classes by ratio.')\n",
    "\n",
    "    split_train = len(files) - sum(fixed)\n",
    "    split_val = split_train + fixed[0]\n",
    "\n",
    "    li = split_files(files, split_train, split_val, len(fixed) == 2)\n",
    "    copy_files(li, class_dir, output)\n",
    "    return len(files)\n",
    "\n",
    "\n",
    "def split_class_dir_ratio(class_dir, output, ratio, seed):\n",
    "    \"\"\"Splits one very class folder\n",
    "    \"\"\"\n",
    "    files #= setup_files(class_dir, seed)\n",
    "\n",
    "    split_train = int(ratio[0] * len(files))\n",
    "    split_val = split_train + int(ratio[1] * len(files))\n",
    "\n",
    "    li = split_files(files, split_train, split_val, len(ratio) == 3)\n",
    "    copy_files(li, class_dir, output)\n",
    "\n",
    "\n",
    "def split_files(files, split_train, split_val):\n",
    "    \"\"\"Splits the files along the provided indices\n",
    "    \"\"\"\n",
    "    files_train = files[:split_train]\n",
    "    files_val = files[split_train:]\n",
    "\n",
    "    li = [(files_train, 'train'), (files_val, 'val')]\n",
    "\n",
    "    return li\n",
    "\n",
    "\n",
    "def copy_files(files_type, class_dir, output):\n",
    "    \"\"\"Copies the files from the input folder to the output folder\n",
    "    \"\"\"\n",
    "    # get the last part within the file\n",
    "    class_name = path.split(class_dir)[1]\n",
    "    for (files, folder_type) in files_type:\n",
    "        full_path = path.join(output, folder_type, class_name)\n",
    "\n",
    "        pathlib.Path(full_path).mkdir(\n",
    "            parents=True, exist_ok=True)\n",
    "        for f in files:\n",
    "            shutil.copy2(f, full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir1 = list_dirs(\"data_files/Cactus_Image/training_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files(\"data_files/Cactus_Image/training_set/cactus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio1 = ratio(\"data_files/Cactus_Image/training_set\",ratio=(0.8, 0.0, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed1 = fixed(\"data_files/Cactus_Image/training_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = setup_files(\"data_files/Cactus_Image/training_set/cactus\", seed=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed = ((100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_class_dir_fixed(\"data_files/Cactus_Image/training_set/cactus\",output=\"output\", ratio = (.8, .0, .1), seed=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_class_dir_ratio(\"data_files/Cactus_Image/training_set/cactus\", \"output\", (.8, .2), seed=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_files(files=setup_files())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-7311d82176c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mclass_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data_files/Cactus_Image/training_set/no_cactus\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlist_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_files/Cactus_Image/training_set/no_cactus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_files/Cactus_Image/training_set/no_cactus\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1337\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msplit_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-058729b8d7d9>\u001b[0m in \u001b[0;36msetup_files\u001b[0;34m(class_dir, seed)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import shutil\n",
    "from os import path, listdir\n",
    "\n",
    "def setup_files(class_dir, seed):\n",
    "    \"\"\"Returns shuffled files\n",
    "    \"\"\"\n",
    "    # make sure its reproducible\n",
    "    random.seed(seed)\n",
    "\n",
    "    files = list_files(class_dir)\n",
    "\n",
    "    files.sort()\n",
    "    random.shuffle(files)\n",
    "    return files\n",
    "\n",
    "\n",
    "def split_files(files, split_train, split_val):\n",
    "    \"\"\"Splits the files along the provided indices\n",
    "    \"\"\"\n",
    "    files_train = files[:split_train]\n",
    "    files_val = files[split_train:]\n",
    "\n",
    "    li = [(files_train, 'train'), (files_val, 'val')]\n",
    "\n",
    "    return li\n",
    "\n",
    "def list_files(directory):\n",
    "    \"\"\"Returns all files in a given directory\n",
    "    \"\"\"\n",
    "    return [f for f in pathlib.Path(directory).iterdir() if f.is_file() and not f.name.startswith('.')]\n",
    "\n",
    "def copy_files(files_type, class_dir, output):\n",
    "    \"\"\"Copies the files from the input folder to the output folder\n",
    "    \"\"\"\n",
    "    # get the last part within the file\n",
    "    class_name = path.split(class_dir)[1]\n",
    "    for (files, folder_type) in files_type:\n",
    "        full_path = path.join(output, folder_type, class_name)\n",
    "\n",
    "        pathlib.Path(full_path).mkdir(\n",
    "            parents=True, exist_ok=True)\n",
    "        for f in files:\n",
    "            shutil.copy2(f, full_path)\n",
    "\n",
    "class_dir = \"data_files/Cactus_Image/training_set/no_cactus\"\n",
    "list_files = list_files(\"data_files/Cactus_Image/training_set/no_cactus\")\n",
    "files = setup_files(\"data_files/Cactus_Image/training_set/no_cactus\",seed=1337)\n",
    "ratio = (0.8, 0.2)\n",
    "split_train = int(ratio[0]*len(files))\n",
    "split_val = split_train + int(ratio[1] * len(files))\n",
    "li = split_files(files, split_train, split_val)\n",
    "copy_files(li, class_dir, \"data_files/Cactus_Image/training_set/no_cactus_new\")\n",
    "\n",
    "split_class_dir_ratio(class_dir=class_dir,\n",
    "                      ratio=ratio,\n",
    "                      output=\"data_files/Cactus_Image/training_set/train_cactus\",\n",
    "                      seed=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_class_dir_ratio(class_dir=class_dir,\n",
    "                      ratio=ratio,\n",
    "                      output=\"data_files/Cactus_Image/training_set/train_cactus\",\n",
    "                      seed=1337)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
