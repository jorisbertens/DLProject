{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import os, shutil\n",
    "import utils\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from ML_algorithms import *\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"data_files/Cactus_Image/training_set\"\n",
    "test_dir = \"data_files/Cactus_Image/testing_set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_cnn(n_neurons=32, n_layers=3, filter_size=(3, 3), activation=\"relu\", \n",
    "               input_shape =(64,64,3), max_pooling=(2,2), dense_layer=128, \n",
    "               loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=\"acc\"):\n",
    "    # NOTE: always alter the input_shape to the specific input shape off the problem.\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(n_neurons, filter_size, activation=activation,\n",
    "                           input_shape =input_shape))\n",
    "    model.add(layers.MaxPooling2D(max_pooling))\n",
    "    for num in range(n_layers-2):\n",
    "        model.add(layers.Conv2D(n_neurons, filter_size, activation=activation))\n",
    "        model.add(layers.MaxPooling2D(max_pooling))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(dense_layer, activation=activation))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[metrics])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = keras_cnn() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(64,64),\n",
    "    batch_size=20,\n",
    "    class_mode=\"binary\")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=20,\n",
    "    class_mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=250,\n",
    "    epochs=5,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Timeseries Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/jovyan/work/2_Semester/Deep Learning/DLProject/utils.py:214: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  X_train = X_train.as_matrix().reshape((len(X_train), 15))\n",
      "/home/jovyan/work/2_Semester/Deep Learning/DLProject/utils.py:215: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  y_train = y_train.as_matrix().reshape((len(y_train), 3))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 2235 into shape (2235,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-18792db31e8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_timeseries_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Perth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_or_lstm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/2_Semester/Deep Learning/DLProject/utils.py\u001b[0m in \u001b[0;36mget_timeseries_dataset\u001b[0;34m(city, cnn_or_lstm)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcnn_or_lstm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 2235 into shape (2235,3)"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, df = utils.get_timeseries_dataset(city = \"Perth\", cnn_or_lstm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = keras_cnn_conv1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv1d_3_input to have shape (3, 15) but got array with shape (1, 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a3838bdee0d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv1d_3_input to have shape (3, 15) but got array with shape (1, 15)"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Titanic Data \n",
    "### 2.1 conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:392: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n",
      "/home/jovyan/work/2_Semester/Deep Learning/DLProject/utils.py:97: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  X_train = X_train.as_matrix().reshape((len(X_train), 36))\n",
      "/home/jovyan/work/2_Semester/Deep Learning/DLProject/utils.py:98: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  y_train = y_train.as_matrix().reshape((len(y_train), 1))\n",
      "/home/jovyan/work/2_Semester/Deep Learning/DLProject/utils.py:99: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  X_test = X_test.as_matrix().reshape((len(X_test), 36))\n",
      "/home/jovyan/work/2_Semester/Deep Learning/DLProject/utils.py:100: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  y_test = y_test.as_matrix().reshape((len(y_test), 1))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = utils.get_titanic_dataset(cnn_or_lstm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = keras_cnn_conv1D(filters=6, input_shape=(1,36), pool_size=1,kernel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1047 samples, validate on 262 samples\n",
      "Epoch 1/20\n",
      "1047/1047 [==============================] - 0s 282us/step - loss: 3.6611 - f1: 0.1797 - val_loss: 4.0590 - val_f1: 0.0000e+00\n",
      "Epoch 2/20\n",
      "1047/1047 [==============================] - 0s 8us/step - loss: 4.0245 - f1: 0.0000e+00 - val_loss: 3.9160 - val_f1: 0.0208\n",
      "Epoch 3/20\n",
      "1047/1047 [==============================] - 0s 7us/step - loss: 3.7440 - f1: 0.0352 - val_loss: 3.7687 - val_f1: 0.1667\n",
      "Epoch 4/20\n",
      "1047/1047 [==============================] - 0s 7us/step - loss: 3.3540 - f1: 0.2056 - val_loss: 3.7750 - val_f1: 0.1549\n",
      "Epoch 5/20\n",
      "1047/1047 [==============================] - 0s 7us/step - loss: 3.3223 - f1: 0.1903 - val_loss: 3.7097 - val_f1: 0.0577\n",
      "Epoch 6/20\n",
      "1047/1047 [==============================] - 0s 8us/step - loss: 3.3006 - f1: 0.0930 - val_loss: 3.6995 - val_f1: 0.0388\n",
      "Epoch 7/20\n",
      "1047/1047 [==============================] - 0s 8us/step - loss: 3.2635 - f1: 0.0991 - val_loss: 3.5838 - val_f1: 0.1440\n",
      "Epoch 8/20\n",
      "1047/1047 [==============================] - 0s 7us/step - loss: 3.1663 - f1: 0.1851 - val_loss: 3.5067 - val_f1: 0.1757\n",
      "Epoch 9/20\n",
      "1047/1047 [==============================] - 0s 7us/step - loss: 3.1394 - f1: 0.2149 - val_loss: 3.4076 - val_f1: 0.1488\n",
      "Epoch 10/20\n",
      "1047/1047 [==============================] - 0s 7us/step - loss: 3.0709 - f1: 0.1861 - val_loss: 3.2223 - val_f1: 0.1709\n",
      "Epoch 11/20\n",
      "1047/1047 [==============================] - 0s 7us/step - loss: 2.9792 - f1: 0.1599 - val_loss: 3.1927 - val_f1: 0.1441\n",
      "Epoch 12/20\n",
      "1047/1047 [==============================] - 0s 8us/step - loss: 2.9486 - f1: 0.1438 - val_loss: 3.1558 - val_f1: 0.2205\n",
      "Epoch 13/20\n",
      "1047/1047 [==============================] - 0s 8us/step - loss: 2.8633 - f1: 0.2326 - val_loss: 3.0518 - val_f1: 0.2424\n",
      "Epoch 14/20\n",
      "1047/1047 [==============================] - 0s 7us/step - loss: 2.7421 - f1: 0.2397 - val_loss: 3.0247 - val_f1: 0.2222\n",
      "Epoch 15/20\n",
      "1047/1047 [==============================] - 0s 8us/step - loss: 2.6674 - f1: 0.2387 - val_loss: 2.9597 - val_f1: 0.2479\n",
      "Epoch 16/20\n",
      "1047/1047 [==============================] - 0s 9us/step - loss: 2.5839 - f1: 0.2049 - val_loss: 2.9301 - val_f1: 0.1818\n",
      "Epoch 17/20\n",
      "1047/1047 [==============================] - 0s 8us/step - loss: 2.5523 - f1: 0.1336 - val_loss: 2.8834 - val_f1: 0.1802\n",
      "Epoch 18/20\n",
      "1047/1047 [==============================] - 0s 8us/step - loss: 2.5239 - f1: 0.1714 - val_loss: 2.8467 - val_f1: 0.1947\n",
      "Epoch 19/20\n",
      "1047/1047 [==============================] - 0s 7us/step - loss: 2.4623 - f1: 0.2038 - val_loss: 2.8726 - val_f1: 0.2707\n",
      "Epoch 20/20\n",
      "1047/1047 [==============================] - 0s 8us/step - loss: 2.3876 - f1: 0.2686 - val_loss: 2.6884 - val_f1: 0.2819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efed0956978>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20,batch_size=512, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:392: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = utils.get_titanic_dataset(cnn_conv2d=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras_cnn(filter_size=(1,1), input_shape=(1, 36, 1), max_pooling=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1047 samples, validate on 262 samples\n",
      "Epoch 1/20\n",
      "1047/1047 [==============================] - 0s 343us/step - loss: 1.0527 - f1: 0.4727 - val_loss: 0.6553 - val_f1: 0.1495\n",
      "Epoch 2/20\n",
      "1047/1047 [==============================] - 0s 40us/step - loss: 0.6394 - f1: 0.3160 - val_loss: 0.6573 - val_f1: 0.3699\n",
      "Epoch 3/20\n",
      "1047/1047 [==============================] - 0s 40us/step - loss: 0.6637 - f1: 0.4257 - val_loss: 0.6826 - val_f1: 0.3810\n",
      "Epoch 4/20\n",
      "1047/1047 [==============================] - 0s 42us/step - loss: 0.6723 - f1: 0.4549 - val_loss: 0.6434 - val_f1: 0.4908\n",
      "Epoch 5/20\n",
      "1047/1047 [==============================] - 0s 43us/step - loss: 0.6435 - f1: 0.5290 - val_loss: 0.6191 - val_f1: 0.5287\n",
      "Epoch 6/20\n",
      "1047/1047 [==============================] - 0s 43us/step - loss: 0.6203 - f1: 0.5532 - val_loss: 0.5767 - val_f1: 0.5244\n",
      "Epoch 7/20\n",
      "1047/1047 [==============================] - 0s 45us/step - loss: 0.5653 - f1: 0.5697 - val_loss: 0.5910 - val_f1: 0.2162\n",
      "Epoch 8/20\n",
      "1047/1047 [==============================] - 0s 43us/step - loss: 0.5867 - f1: 0.2686 - val_loss: 0.5673 - val_f1: 0.4697\n",
      "Epoch 9/20\n",
      "1047/1047 [==============================] - 0s 42us/step - loss: 0.5519 - f1: 0.5799 - val_loss: 0.5523 - val_f1: 0.6243\n",
      "Epoch 10/20\n",
      "1047/1047 [==============================] - 0s 43us/step - loss: 0.5464 - f1: 0.6711 - val_loss: 0.5398 - val_f1: 0.6022\n",
      "Epoch 11/20\n",
      "1047/1047 [==============================] - 0s 46us/step - loss: 0.5289 - f1: 0.6575 - val_loss: 0.5222 - val_f1: 0.5839\n",
      "Epoch 12/20\n",
      "1047/1047 [==============================] - 0s 46us/step - loss: 0.5091 - f1: 0.6565 - val_loss: 0.5150 - val_f1: 0.5806\n",
      "Epoch 13/20\n",
      "1047/1047 [==============================] - 0s 43us/step - loss: 0.4996 - f1: 0.6519 - val_loss: 0.5074 - val_f1: 0.5875\n",
      "Epoch 14/20\n",
      "1047/1047 [==============================] - 0s 41us/step - loss: 0.4907 - f1: 0.6744 - val_loss: 0.5001 - val_f1: 0.6335\n",
      "Epoch 15/20\n",
      "1047/1047 [==============================] - 0s 42us/step - loss: 0.4846 - f1: 0.7072 - val_loss: 0.4939 - val_f1: 0.6784\n",
      "Epoch 16/20\n",
      "1047/1047 [==============================] - 0s 42us/step - loss: 0.4762 - f1: 0.7141 - val_loss: 0.4876 - val_f1: 0.6667\n",
      "Epoch 17/20\n",
      "1047/1047 [==============================] - 0s 47us/step - loss: 0.4675 - f1: 0.7116 - val_loss: 0.4906 - val_f1: 0.6289\n",
      "Epoch 18/20\n",
      "1047/1047 [==============================] - 0s 44us/step - loss: 0.4708 - f1: 0.6930 - val_loss: 0.4887 - val_f1: 0.6258\n",
      "Epoch 19/20\n",
      "1047/1047 [==============================] - 0s 41us/step - loss: 0.4613 - f1: 0.7145 - val_loss: 0.4829 - val_f1: 0.6809\n",
      "Epoch 20/20\n",
      "1047/1047 [==============================] - 0s 44us/step - loss: 0.4566 - f1: 0.7256 - val_loss: 0.4800 - val_f1: 0.6893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efed0395f60>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20,batch_size=512, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_rows, img_cols = 1,36\n",
    "#nb_filters = 1000\n",
    "#pool_size = (1, 1)\n",
    "#kernel_size = (1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Data \n",
    "### 2.1 conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = utils.get_text_dataset(cnn_or_lstm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras_cnn_conv1D(filters=24, input_shape=(1,51), pool_size=1,kernel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=20, batch_size=512, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = utils.get_text_dataset(cnn_conv2d=True)\n",
    "\n",
    "model = keras_cnn(filter_size=(1,1), input_shape=(1, 51, 1), max_pooling=(1,1))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=512, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big dataset\n",
    "### 2.1 conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = utils.get_bank_dataset(cnn_or_lstm=True)\n",
    "\n",
    "model = keras_cnn_conv1D(filters=6, input_shape=(1,29), pool_size=1,kernel_size=1)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=512, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = utils.get_bank_dataset(cnn_conv2d=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = keras_cnn(filter_size=(1,1), input_shape=(1, 29, 1), max_pooling=(1,1))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=512, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation, Dropout, Convolution2D, MaxPooling2D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(nb_filters, filter_size=(1,1),\n",
    "                        border_mode='valid',\n",
    "                        input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='mean_squared_error', optimizer='Nadam', metrics=['binary_accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_onehot, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation, Dropout, Convolution2D, MaxPooling2D, Flatten\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, filter_size=(1,1), activation=\"relu\", input_shape=input_shape))\n",
    "model.add(layers.MaxPooling2D(pool_size))\n",
    "model.add(layers.Conv2D(32, filter_size=(1,1), activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(pool_size))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(dense_layer, activation=activation))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='Nadam', metrics=['binary_accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_onehot, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.as_matrix().reshape((len(X_train), 15))\n",
    "y_train = y_train.as_matrix().reshape((len(y_train), 1))\n",
    "X_test = X_test.as_matrix().reshape((len(X_test), 15))\n",
    "y_test = y_test.as_matrix().reshape((len(y_test), 1))\n",
    "\n",
    "train_dataset=hstack((X_train,y_train))\n",
    "test_dataset=hstack((X_test,y_test))\n",
    "\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "X_train, y_train = split_sequences(train_dataset, 3)\n",
    "X_test, y_test = split_sequences(test_dataset, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_cnn_conv1D(filters=64, n_layers=2, kernel_size=2, activation=\"relu\", \n",
    "               input_shape =(3,15), pool_size=2, dense_layer=50, \n",
    "               loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=\"acc\"):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=activation, input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    for num in range(n_layers-2):\n",
    "        model.add(Conv1D(filters=filters, kernel_size=kernen_size, activation=activation, input_shape=input_shape))\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dense_layer, activation=activation))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=[metrics])\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
